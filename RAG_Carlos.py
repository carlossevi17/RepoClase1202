import streamlit as st
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import os

# ConfiguraciÃ³n de la interfaz
st.set_page_config(page_title="SafeBank AI Reader", page_icon="ðŸ“–")
st.title("ðŸ“– Analizador de Manuales SafeBank")

# Barra lateral para configuraciÃ³n
with st.sidebar:
    st.header("ConfiguraciÃ³n")
    api_key = st.text_input("Introduce tu Groq API Key:", type="password")
    modelo = st.selectbox("Modelo", ["llama-3.3-70b-versatile", "mixtral-8x7b-32768"])
    archivo = st.file_uploader("Sube el PDF del manual", type="pdf")

# InicializaciÃ³n del sistema RAG
if archivo and api_key:
    # Guardar el PDF temporalmente para que el Loader pueda leerlo
    with open("temp_manual.pdf", "wb") as f:
        f.write(archivo.getbuffer())

    # Procesamiento del documento
    @st.cache_resource # Esto evita que se procese el PDF cada vez que haces una pregunta
    def procesar_pdf(ruta):
        loader = PyMuPDFLoader(ruta)
        docs = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)
        chunks = text_splitter.split_documents(docs)
        embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
        vectorstore = FAISS.from_documents(chunks, embeddings)
        return vectorstore.as_retriever(search_kwargs={"k": 3})

    retriever = procesar_pdf("temp_manual.pdf")
    st.success("âœ… Manual analizado y listo para preguntas.")

    # Interfaz de Chat
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if pregunta := st.chat_input("Â¿QuÃ© quieres saber del manual?"):
        st.session_state.messages.append({"role": "user", "content": pregunta})
        st.chat_message("user").write(pregunta)

        # BÃºsqueda y GeneraciÃ³n
        contexto_docs = retriever.invoke(pregunta)
        contexto_texto = "\n\n".join([doc.page_content for doc in contexto_docs])
        
        # Prompt directo (mÃ¡s estable que las cadenas pre-hechas)
        template = ChatPromptTemplate.from_messages([
            ("system", "Responde basÃ¡ndote solo en este contexto:\n\n{context}"),
            ("human", "{input}")
        ])
        
        llm = ChatGroq(groq_api_key=api_key, model=modelo, temperature=0.3)
        chain = template | llm | StrOutputParser()
        
        respuesta = chain.invoke({"context": contexto_texto, "input": pregunta})
        
        st.session_state.messages.append({"role": "assistant", "content": respuesta})
        st.chat_message("assistant").write(respuesta)
else:
    st.info("Por favor, introduce tu API Key y sube un archivo PDF para comenzar.")
